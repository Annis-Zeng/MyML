{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dataset_tutorials.ipynb","version":"0.3.2","provenance":[{"file_id":"1v5Psy1Wxo6tGCOJ0aThgWMQVdMpN0p9g","timestamp":1557923969615}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5T1hCe9QDzm0","colab_type":"text"},"source":["# 初始化"]},{"cell_type":"code","metadata":{"id":"1YsugnXkESxO","colab_type":"code","colab":{}},"source":["#@markdown - **挂载** \n","from google.colab import drive\n","drive.mount('GoogleDrive')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1A2EJlwVE3Ku","colab_type":"code","colab":{}},"source":["# #@markdown - **卸载**\n","# !fusermount -u GoogleDrive"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kM26SWRvFbZd","colab_type":"text"},"source":["# 代码区"]},{"cell_type":"code","metadata":{"id":"T3cqJ1tcFfXq","colab_type":"code","colab":{}},"source":["#@title TensorFlow Dataset 类的使用 { display-mode: \"both\" }\n","# 该程序简要的介绍了 TensorFlow 中 Dataset 类的使用\n","# Dataset tutorials\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8p9TxifRF-gD","colab_type":"code","colab":{}},"source":["#@markdown - **参数设置**\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","initial_rate = 1e-3 #@param {type: \"number\"}\n","scale_factor = 60000 #@param {type: \"number\"}, scale factor of l2 regularization, default is 3000\n","\n","batchsize = 256 #@param {type: \"integer\"}, batch size, default is 128\n","num_epochs = 6000 #@param {type: \"integer\"}\n","\n","event_path = './Tensorboard' #@param {type: \"string\"}\n","checkpoint_path = './Checkpoints' #@param {type: \"string\"}\n","# online_test = True #@param {type: \"boolean\"}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ZK2ktl3LbAI","colab_type":"code","colab":{}},"source":["#@markdown - **载入图像数据**\n","mnist_train = tfds.as_numpy(tfds.load(\"mnist\", split=tfds.Split.TRAIN, batch_size=-1))\n","imgs_train, labels_train = mnist_train['image'] / 255., mnist_train['label']\n","mnist_test = tfds.as_numpy(tfds.load(\"mnist\", split=tfds.Split.TEST, batch_size=-1))\n","imgs_test, labels_test = mnist_test['image'] / 255., mnist_test['label']\n","num_samples = labels_test.shape[0]\n","assert imgs_train.max() == 1., \"warning: 'The value of the pixel is preferably between 0 and 1.'\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqJ3DfvkL2JU","colab_type":"code","colab":{}},"source":["#@markdown - **网络图设置**\n","graph = tf.Graph()\n","with graph.as_default():\n","    \n","    # 指数衰减型 learning_rate\n","    global_step = tf.Variable(0, name='global_step', trainable=False)\n","    decay_steps = 400\n","    decay_rate = 0.90\n","    learning_rate = tf.train.exponential_decay(initial_rate,\n","                                                global_step=global_step,\n","                                                decay_steps=decay_steps,\n","                                                decay_rate=decay_rate,\n","                                                staircase=True,\n","                                                name='learning_rate')\n","    \n","    #@markdown - **迭代器设置**\n","    x_p = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='input_images')\n","    y_p = tf.placeholder(tf.int64, shape=[None, ], name='labels')\n","    batch_size = tf.placeholder(tf.int64, name='batch_size')\n","    data = tf.data.Dataset.from_tensor_slices((x_p, y_p))\n","    data = data.repeat()\n","    data_batch = data.shuffle(2).batch(batch_size).prefetch(batch_size)\n","    iterator = data_batch.make_initializable_iterator()\n","\n","    with tf.name_scope('Input'):\n","        x_input, y_input = iterator.get_next()\n","        y = tf.one_hot(y_input, depth=10)\n","        keep_pro = tf.placeholder(tf.float32)\n","        \n","    with tf.name_scope('Conv1'):\n","        w_1 = tf.Variable(tf.truncated_normal([3, 3, 1, 32], stddev=0.1), name='weights_conv1')\n","        b_1 = tf.Variable(tf.constant(0.1, shape=[32]), name='bias_conv1')\n","        h_conv1 = tf.nn.relu(tf.nn.conv2d(x_input, w_1, strides=[1, 1, 1, 1], padding='SAME') + b_1)\n","        h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    with tf.name_scope('Conv2'):\n","        w_2 = tf.Variable(tf.truncated_normal([3, 3, 32, 64], stddev=0.1), name='weights_conv2')\n","        b_2 = tf.Variable(tf.constant(0.1, shape=[64]), name='bias_conv2')\n","        h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, w_2, strides=[1, 1, 1, 1], padding='SAME') + b_2)\n","        h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    with tf.name_scope('FC1'):\n","        h_pool2_fla = tf.layers.flatten(h_pool2)\n","        num_f = h_pool2_fla.get_shape().as_list()[-1]\n","    \n","        w_fc1 = tf.Variable(tf.truncated_normal([num_f, 128], stddev=0.1), name='weights_fc1')\n","        b_fc1 = tf.Variable(tf.constant(0.1, shape=[128]), name='bias_fc1')\n","        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_fla, w_fc1) + b_fc1)\n","        h_drop1 = tf.nn.dropout(h_fc1, keep_prob=keep_pro, name='Dropout')\n","\n","    with tf.name_scope('Output'):\n","        w_fc2 = tf.Variable(tf.truncated_normal([128, 10], stddev=0.1), name='weights_fc2')\n","        b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]), name='bias_fc2')\n","        h_fc2 = tf.matmul(h_drop1, w_fc2) + b_fc2\n","\n","    #@markdown - **L2 正则化**\n","    tf.add_to_collection(tf.GraphKeys.WEIGHTS, w_fc1)\n","    tf.add_to_collection(tf.GraphKeys.WEIGHTS, w_fc2)\n","    regularizer = tf.contrib.layers.l2_regularizer(scale=15./scale_factor)\n","    reg_tem = tf.contrib.layers.apply_regularization(regularizer)\n","\n","    with tf.name_scope('loss'):\n","        entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=h_fc2))\n","        entropy_loss = tf.add(entropy_loss, reg_tem, name='l2_loss')\n","    \n","    with tf.name_scope('accuracy'):\n","        prediction = tf.cast(tf.equal(tf.arg_max(h_fc2, 1), tf.argmax(y, 1)), \"float\")\n","        accuracy = tf.reduce_mean(prediction)\n","    \n","    with tf.name_scope('train'):\n","        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","        train_op = optimizer.minimize(entropy_loss, global_step=global_step)\n","    \n","    #@markdown - **summary 设置**\n","    tf.summary.image('input_images', x_input, max_outputs=3, collections=['train'])\n","    tf.summary.histogram('conv1_weights', w_1, collections=['train'])\n","    tf.summary.histogram('conv1_bias', b_1, collections=['train'])\n","    tf.summary.scalar('loss', entropy_loss, collections=['train'])\n","    tf.summary.scalar('accuracy', accuracy, collections=['train'])\n","    tf.summary.scalar('global_step', global_step, collections=['train'])\n","    tf.summary.scalar('learning_rate', learning_rate, collections=['train'])\n","\n","    summ_train = tf.summary.merge_all('train')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uolZRjNtqPC1","colab_type":"code","outputId":"2aaef157-33f8-4ca9-b0d4-596f4da65b81","executionInfo":{"status":"ok","timestamp":1557930390968,"user_tz":-480,"elapsed":94291,"user":{"displayName":"Лянпэн К","photoUrl":"https://lh6.googleusercontent.com/-GXVG-PbMfAw/AAAAAAAAAAI/AAAAAAAAADo/wvm2q-yqQzs/s64/photo.jpg","userId":"04289897042674768581"}},"colab":{"base_uri":"https://localhost:8080/","height":890}},"source":["#@markdown - **网络训练**\n","max_acc = 100.\n","min_loss = 0.1\n","with tf.Session(graph=graph) as sess:\n","    sess.run(tf.global_variables_initializer())\n","    sess.run(iterator.initializer, feed_dict={x_p: imgs_train, y_p: labels_train, batch_size: batchsize})\n","\n","    summ_train_dir = os.path.join(event_path, 'summaries','train')\n","    summ_train_Writer = tf.summary.FileWriter(summ_train_dir)\n","    summ_train_Writer.add_graph(sess.graph)\n","\n","    saver = tf.train.Saver(max_to_keep=1)\n","    print('Training ========== (。・`ω´・) ========\\n')\n","\n","    for num in range(num_epochs):\n","\n","        _, acc, loss, rs = sess.run([train_op, accuracy, entropy_loss, summ_train], feed_dict={keep_pro: 0.5, \n","                                                                                                batch_size: batchsize})\n","        summ_train_Writer.add_summary(rs, global_step=num)\n","        acc *= 100\n","        num_e = str(num + 1)\n","        print_list = [num_e, loss, acc]\n","        if (num + 1) % 400 == 0:\n","            print('Keep on training ========== (。・`ω´・) ========')\n","            print('Epoch {0[0]}, train_loss is {0[1]:.4f}, accuracy is {0[2]:.2f}%.\\n'.format(print_list))\n","    print('\\n', 'Training completed.')\n","    \n","    #@markdown - **测试集**\n","    print('Testing ========== (。・`ω´・) ========')\n","    sess.run(iterator.initializer, feed_dict={x_p: imgs_test, y_p: labels_test, batch_size: num_samples})\n","    acc, loss = sess.run([accuracy, entropy_loss], feed_dict={keep_pro: 1., batch_size: num_samples})\n","    acc *= 100\n","    print_list = [loss, acc]\n","    print('Test_loss is {0[0]:.4f}, accuracy is {0[1]:.2f}%.\\n'.format(print_list))\n","                \n","summ_train_Writer.close()\n","sess.close()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training ========== (。・`ω´・) ========\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 400, train_loss is 0.4288, accuracy is 93.36%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 800, train_loss is 0.2347, accuracy is 96.88%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 1200, train_loss is 0.2685, accuracy is 96.88%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 1600, train_loss is 0.1516, accuracy is 98.44%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 2000, train_loss is 0.1211, accuracy is 98.44%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 2400, train_loss is 0.0927, accuracy is 99.61%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 2800, train_loss is 0.0933, accuracy is 98.83%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 3200, train_loss is 0.0916, accuracy is 99.61%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 3600, train_loss is 0.0763, accuracy is 98.83%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 4000, train_loss is 0.0673, accuracy is 98.44%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 4400, train_loss is 0.0793, accuracy is 99.22%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 4800, train_loss is 0.0814, accuracy is 98.44%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 5200, train_loss is 0.0411, accuracy is 100.00%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 5600, train_loss is 0.0401, accuracy is 100.00%.\n","\n","Keep on training ========== (。・`ω´・) ========\n","Epoch 6000, train_loss is 0.0420, accuracy is 100.00%.\n","\n","\n"," Training completed.\n","Testing ========== (。・`ω´・) ========\n","Test_loss is 0.0528, accuracy is 99.24%.\n","\n"],"name":"stdout"}]}]}